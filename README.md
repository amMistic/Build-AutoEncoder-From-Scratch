# Build AutoEncoder From Scratch
This project implements Fully Connected and Convolutional AutoEncoders using PyTorch for MNIST image reconstruction. The Fully Connected AutoEncoder maps images to a 3D latent space, while the ConvAutoEncoder leverages convolutional layers for improved feature extraction. Both models demonstrate strong performance, with clear visual reconstructions and effective training strategies.

## Autoencoder Performance Comparison

**1ï¸âƒ£ AutoEncoder (Fully Connected - Trained for 20 Epochs)**
<p align="center">
    <img src="https://github.com/user-attachments/assets/3d58bf6b-9779-4b46-818c-4a6c7f4f569b" width="45%">
    <img src="https://github.com/user-attachments/assets/3d5380ef-fced-421a-9326-9937836aeb23" width="45%">
</p>

---

**2ï¸âƒ£ ConvAutoEncoder (Convolutional - Trained for 10 Epochs)**
<p align="center">
    <img src="https://github.com/user-attachments/assets/ae203527-1675-45bf-9469-e4cee04ae3dd" width="45%">
    <img src="https://github.com/user-attachments/assets/aa7bcf1e-4045-4b2f-9a2f-d884bed627fa" width="45%">
</p>

---

This project implements two types of autoencoders using PyTorch:
1. **Fully Connected AutoEncoder**
2. **Convolutional AutoEncoder (ConvAutoEncoder)**

Both models are trained using the MNIST dataset to reconstruct digit images.

---

## ğŸš€ Installation
1. Install the dependencies directly within the Jupyter Notebook:
```python
!pip install torch torchvision pandas matplotlib
```

---

## ğŸ§  Models Overview
### 1ï¸âƒ£ AutoEncoder (Fully Connected)
- Encoder compresses 28x28 input into a **3-dimensional** latent space.
- Decoder reconstructs the original input from the compressed representation.

**Encoder Architecture:**
- Linear layers with ReLU activation.
- Decreasing layer sizes: `784 â†’ 128 â†’ 64 â†’ 32 â†’ 16 â†’ 12 â†’ 3`

**Decoder Architecture:**
- Linear layers with ReLU activation and a final Tanh layer.
- Increasing layer sizes: `3 â†’ 12 â†’ 16 â†’ 32 â†’ 64 â†’ 128 â†’ 784`

### 2ï¸âƒ£ ConvAutoEncoder (Convolutional)
- Encoder uses convolution layers with BatchNorm for feature extraction.
- Decoder uses ConvTranspose layers for reconstruction.

**Encoder Architecture:**
- `Conv2d â†’ BatchNorm2d â†’ ReLU` layers with reducing spatial dimensions.

**Decoder Architecture:**
- `ConvTranspose2d â†’ BatchNorm2d â†’ ReLU` layers with increasing spatial dimensions.

---

## ğŸ“Š Dataset Preparation
The dataset used is **MNIST**, provided in CSV format.
- **Features:** Pixel values (0-255) scaled to `[0, 1]`.
- **Labels:** Digits (0-9), used for dataset structure but not for training.

**Sample Transformation:**
```python
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5), (0.5))
])
```

---

## ğŸ”¥ Training Procedure
### Training Parameters
- **Loss Function:** Mean Squared Error (MSE)
- **Optimizer:** Adam
- **Learning Rate:** 1e-3
- **Weight Decay:** 1e-5
- **Batch Size:** 64

### Training Execution
Run each code block in sequence within the Jupyter Notebook to train the models.

---

## ğŸ“ˆ Visualizing Results
Use the provided visualization code to display original and reconstructed images:
```python
visualize_outputs(outputs1, 5)
visualize_outputs(outputs, 5)
```

---

## ğŸ“‹ Results
- Fully Connected AutoEncoder achieves weak reconstruction on digit images.
- ConvAutoEncoder demonstrates sharper and more accurate reconstructions.

---

## ğŸ§© Future Enhancements
- Add deeper architectures for improved accuracy.
- Introduce variational autoencoders (VAE) for better latent space learning.
- Implement custom data augmentation for robustness.
- 
---

## ğŸ™Œ Acknowledgements
- **PyTorch** â€” For efficient deep learning model implementation.
- **MNIST Dataset** â€” For digit image recognition tasks.

